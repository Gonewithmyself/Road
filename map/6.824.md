### 简介

##### 为什么需要分布式系统

- 性能需求
- 容错
- 物理上分布，重庆银行 转账 成都银行
- 安全、隔离

##### 分布式系统挑战

- 并发
- 局部出错
- 性能达到预期

三种基础架构

- 存储
- 计算
- 通信

工具

- RPC
- 线程
- 并发控制

三个指标

- 可扩展性（性能）
- 可用性（容错）
- 一致性（共识）

### MapReduce

##### 背景

进行TB级别的计算任务，如网页排序，词频统计。

使用者只需编写简单的Map、Reduce函数，将分布式系统的复杂性隐藏在MapReduce框架中

##### 流程

1TB文件 M个Map  R个Reduce

1. master将文件拆为M个split分发给worker
2. Map worker 读取一个split 预处理后调用用户map函数
3. 将中间结果hash到R个中间文件中，并将信息传回master
4. map阶段完成后，master将同一个hash索引的中间文件打包成reduce任务派给worker节点
5. reduce阶段 将m个中间文件中相同key放入一个数组，调用用户reduce函数并将结果返回
6. Master节点将 R个文件进行merge操作得到最终结果

##### 容错

- worker fail

  master周期性ping worker，未响应则将其负担任务重新分配一个worker

- master fail

  GG 重新执行整个任务。

##### 性能

- 网络带宽

  尽量使用文件位置相邻的worker

- 落伍者

  启用备用任务

### GFS

##### 架构

  master-worker![GFSarc](G:\github\Road\map\GFSarc.jpg)

  适用大文件存储

64MB chunk为单位分割文件 保存到chunkserver

```go
// snapshot + log 方式进行故障恢复
type master struct{
	file2chunks map[string][]int64 // filename->chunk ids 需要落盘
	chunks map[int64]*chunk        // 不需落盘
}

type chunk struct{
	primarychunk	int64	// 
    lease       	int64
	chunks			[]chunksvr //
}

type chunksvr struct {
    id		int32
    version int64
}

// 
func (m *master)read(fname string, pos int) {
	ids := m.file2chunks[fname]
	idx := pos % 64
	return ids[idx].chunks
}
```



![GFSwrite](G:\github\Road\map\GFSwrite.jpg)

##### 一致性模型

宽松的一致性模型（弱一致性）无法保证chunk的所有副本一致

为何如此设计：

搜索引擎20000个结果 丢了一条或者排序错误，没有人会注意到。